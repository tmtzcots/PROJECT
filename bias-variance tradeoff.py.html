#!/usr/bin/env python
# coding: utf-8

# In[17]:


import numpy as np
import scipy.sparse as sp
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.utils import resample

np.random.seed(2020)

import warnings
# Comment this to turn off warnings 
warnings.filterwarnings('ignore')

### define Ising model aprams
# system size
L=40

# create 10000 random Ising states
states=np.random.choice([-1, 1], size=(10000,L))
print(states)
print(states.shape)

def ising_energies(states):
    """
    This function calculates the energies of the states in the nn Ising Hamiltonian
    """
    L = states.shape[1]
    J = np.zeros((L, L))
    for i in range(L): 
        J[i,(i+1)%L]=-1.0 # interaction between nearest-neighbors
        
    # compute energies
    E = np.einsum('...i,ij,...j->...',states,J,states)

    return E
# calculate Ising energies
energies=ising_energies(states)
print(energies)
print(energies.shape)


# In[18]:


# reshape Ising states into RL samples: S_iS_j --> X_p
states=np.einsum('...i,...j->...ij', states, states)
shape=states.shape
states=states.reshape((shape[0],shape[1]*shape[2]))
# build final data set
Data=[states,energies]


# In[19]:


# define number of samples
n_samples=800
# define train and test data sets
x_train=Data[0][:n_samples]  # 0 - 400
y_train=Data[1][:n_samples] #+ np.random.normal(0,4.0,size=X_train.shape[0])
x_test=Data[0][n_samples:3*n_samples//2]# 400 - 600
y_test=Data[1][n_samples:3*n_samples//2] #+ np.random.normal(0,4.0,size=X_test.shape[0])


# In[20]:


from sklearn import linear_model
import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import seaborn
get_ipython().run_line_magic('matplotlib', 'inline')

# set up Lasso and Ridge Regression models
leastsq=linear_model.LinearRegression()
ridge=linear_model.Ridge()
lasso = linear_model.Lasso()

# define error lists
error_ls = []
bias_ls = []
variance_ls = []

error_ridge = []
bias_ridge = []
variance_ridge = []

error_lasso = []
bias_lasso = []
variance_lasso = []

# set regularisation strength values 规范化长度值
lmbdas = np.logspace(-4, 5, 10)  # lmbda = 10^-4, 10^-3, .... 10^5.


# In[23]:


import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.utils import resample

np.random.seed(2020)

n = 40
n_boostraps = 100


for lmbda in lmbdas:
    model = make_pipeline(PolynomialFeatures(degree=lmbda), LinearRegression(fit_intercept=False))
    y_pred = np.empty((y_test.shape[0], n_boostraps))
    for i in range(n_boostraps):
        x_, y_ = resample(x_train, y_train)
        y_pred[:, i] = model.fit(x_, y_).predict(x_test).ravel()

    polylmbda[lmbda]=lmbda
   
    # use the coefficient of determination R^2 as the performance of prediction.
    error_ls[lmbda] = np.mean( np.mean((y_test - y_pred)**2, axis=1, keepdims=True) )
    bias_ls[lmbda] = np.mean( (y_test - np.mean(y_pred, axis=1, keepdims=True))**2 )
    variance_ls[lmbda] = np.mean( np.var(y_pred, axis=1, keepdims=True) )
    error_ls.append(error_ls[lmbda])
    bias_ls.append(bias_ls[lmbda])
    variance_ls.append(variance_ls[lmbda])
    
    ### apply RIDGE regression
    ridge.set_params(alpha=lmbda) # set regularisation parameter
    ridge.fit(x_train, y_train) # fit model 
  
   
    
    ### apply LASSO regression
    lasso.set_params(alpha=lmbda) # set regularisation parameter
    lasso.fit(x_train, y_train) # fit model
  
    
  
    print('lmbda:', lmbda)
    print('Error:', error_ls[lmbda])
    print('Bias^2:', bias_ls[lmbda])
    print('Var:', variance_ls[lmbda])
    #print('{} >= {} + {} = {}'.format(error[degree], bias[degree], variance[degree], bias[degree]+variance[degree]))

plt.plot(polylmbda, error_ls, label='Error')
plt.plot(polylmbda, bias_ls, label='bias')
plt.plot(polylmbda, variance_ls, label='Variance')
plt.legend()
plt.show()


# In[22]:


# Plot our performance on both the training and test data
plt.semilogx(lmbdas, errors_leastsq, 'b',label='Train (OLS)')
plt.semilogx(lmbdas, bias_leastsq,'--b',label='Test (OLS)')
plt.semilogx(lmbdas, variance_leastsq,'--b',label='Test (OLS)')
plt.semilogx(lmbdas, errors_ridge,'r',label='Train (Ridge)',linewidth=1)
plt.semilogx(lmbdas, bias_ridge,'--r',label='Test (Ridge)',linewidth=1)
plt.semilogx(lmbdas, variance_ridge,'--b',label='Test (OLS)')
plt.semilogx(lmbdas, errors_lasso, 'g',label='Train (LASSO)')
plt.semilogx(lmbdas, bias_lasso, '--g',label='Test (LASSO)')
plt.semilogx(lmbdas, variance_lasso, '--g',label='Test (LASSO)')

fig = plt.gcf()
fig.set_size_inches(10.0, 6.0)

#plt.vlines(alpha_optim, plt.ylim()[0], np.max(test_errors), color='k',
#           linewidth=3, label='Optimum on test')
plt.legend(loc='lower left',fontsize=16)
plt.ylim([-0.1, 1.1])
plt.xlim([min(lmbdas), max(lmbdas)])
plt.xlabel(r'$\lambda$',fontsize=16)
plt.ylabel('Performance',fontsize=16)
plt.tick_params(labelsize=16)

plt.show()


# In[ ]:





# In[ ]:




